{"cells":[{"cell_type":"markdown","metadata":{"id":"P6fdbxQu_8JX"},"source":["##Training SSD on a custom dataset"]},{"cell_type":"markdown","metadata":{"id":"sgaW_eoGACEi"},"source":["##1.Download the image dataset and clone the GitHub repository hosting the code for the model and other utilities for processing the data."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52643,"status":"ok","timestamp":1729852485639,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"-dkmXq0J5ayF","outputId":"826c28e0-4a0b-4335-c43a-aa3f6279bad5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ssd-utils'...\n","remote: Enumerating objects: 9, done.\u001b[K\n","remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n","remote: Compressing objects:  12% (1/8)\u001b[K\rremote: Compressing objects:  25% (2/8)\u001b[K\rremote: Compressing objects:  37% (3/8)\u001b[K\rremote: Compressing objects:  50% (4/8)\u001b[K\rremote: Compressing objects:  62% (5/8)\u001b[K\rremote: Compressing objects:  75% (6/8)\u001b[K\rremote: Compressing objects:  87% (7/8)\u001b[K\rremote: Compressing objects: 100% (8/8)\u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n","Receiving objects:  11% (1/9)\rReceiving objects:  22% (2/9)\rReceiving objects:  33% (3/9)\rReceiving objects:  44% (4/9)\rReceiving objects:  55% (5/9)\rReceiving objects:  66% (6/9)\rReceiving objects:  77% (7/9)\rReceiving objects:  88% (8/9)\rReceiving objects: 100% (9/9)\rReceiving objects: 100% (9/9), 13.65 KiB | 13.65 MiB/s, done.\n","remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","/content/ssd-utils/ssd-utils/ssd-utils/ssd-utils\n"]}],"source":["import os\n","if not os.path.exists('open-images-bus-trucks'):\n","  %pip install -q torch_snippets\n","  !wget --quiet https://www.dropbox.com/s/agmzwk95v96ihic/open-images-bus-trucks.tar.xz\n","  !tar -xf open-images-bus-trucks.tar.xz\n","  !rm open-images-bus-trucks.tar.xz\n","  !git clone https://github.com/sizhky/ssd-utils/\n","%cd ssd-utils"]},{"cell_type":"markdown","metadata":{"id":"aVRBb4u4DVmR"},"source":["##2.Pre-process the data"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1729852485640,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"4Qk1ODUmDDc-"},"outputs":[],"source":["from torch_snippets import *\n","import glob\n","import torch\n","\n","\n","DATA_ROOT = \"../open-images-bus-trucks/\"\n","IMAGE_ROOT = f\"{DATA_ROOT}/images\"\n","DF_RAW = df = pd.read_csv(f\"{DATA_ROOT}/df.csv\")\n","\n","df = df[df['ImageID'].isin(df['ImageID'].unique().tolist())]\n","\n","label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n","label2target['background'] = 0\n","target2label = {t:l for l,t in label2target.items()}\n","background_class = len(label2target)\n","num_classes = len(label2target)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"OSa3JymxEmyz"},"source":["##3.Prepare the dataset class"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":516,"status":"ok","timestamp":1729852663676,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"WHtuMbq-Ep4E"},"outputs":[],"source":["import collections, os, torch\n","from PIL import Image\n","from torchvision import transforms\n","\n","normalize = transforms.Normalize(\n","  mean=[0.485, 0.456, 0.406],\n","  std=[0.229, 0.224, 0.225]\n",")\n","\n","denormalize = transforms.Normalize(\n","  mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n","  std=[1/0.229, 1/0.224, 1/0.225]\n",")\n","\n","\n","def preprocess_image(img):\n","  img = torch.tensor(img).permute(2,0,1)\n","  img = normalize(img)\n","  return img.to(device).float()\n","\n","\n","\n","class OpenDataset(torch.utils.data.Dataset):\n","  w, h = 300, 300\n","  def __init__(self, df, image_dir=IMAGE_ROOT):\n","    self.image_dir = image_dir\n","    self.files = glob.glob(self.image_dir+'/*')\n","    self.df = df\n","    self.image_infos = df.ImageID.unique()\n","    logger.info(f'{len(self)} items loaded')\n","\n","  def __getitem__(self, ix):\n","    # load images and masks\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n","    data = df[df['ImageID'] == image_id]\n","    labels = data['LabelName'].values.tolist()\n","    data = data[['XMin', 'YMin', 'XMax','YMax']].values\n","    data[:, [0,2]]*= self.w\n","    data[:, [1,3]]*= self.h\n","    boxes  = data.astype(np.uint32).tolist()  # convert to absolute coordinates\n","    return img, boxes, labels\n","\n","  def collate_fn(self, batch):\n","    images, boxes, labels = [], [], []\n","    for item in batch:\n","      img, image_boxes, image_labels = item\n","      img = preprocess_image(img)[None]\n","      images.append(img)\n","      boxes.append(torch.tensor(image_boxes).float().to(device)/300.)\n","      labels.append(torch.tensor([label2target[c] for c in image_labels]).long().to(device))\n","\n","    images = torch.cat(images).to(device)\n","    return images, boxes, labels\n","\n","\n","  def __len__(self):\n","    return len(self.image_infos)\n"]},{"cell_type":"markdown","metadata":{"id":"md2Asi8oItqc"},"source":["##4.Prepare the training and test datasets and the dataloaders"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":78},"executionInfo":{"elapsed":370,"status":"ok","timestamp":1729852667852,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"HZE4EhZrIqvx","outputId":"a1718de6-3ab5-4bfe-f8a9-30ac38f473de"},"outputs":[{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\"\u003e[10/25/24 10:37:47] \u003c/span\u003e\u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003eINFO    \u003c/span\u003e \u003cspan style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"\u003e13702\u003c/span\u003e items loaded                                                                                  \u003ca href=\"file://\u003cipython-input-34-a1e57a40ea70\u003e:30\" target=\"_blank\"\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e\u0026lt;ipython-input-34-a1e57a40ea70\u0026gt;\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e:\u003c/span\u003e\u003ca href=\"file://\u003cipython-input-34-a1e57a40ea70\u003e:30#__init__:30\" target=\"_blank\"\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e__init__:30\u003c/span\u003e\u003c/a\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[2;36m[10/25/24 10:37:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;33mINFO    \u001b[0m \u001b[1;36m13702\u001b[0m items loaded                                                                                  \u001b]8;id=304562;file://\u003cipython-input-34-a1e57a40ea70\u003e:30\u001b\\\u001b[2m\u003cipython-input-34-a1e57a40ea70\u003e\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=646115;file://\u003cipython-input-34-a1e57a40ea70\u003e:30#__init__:30\u001b\\\u001b[2m__init__:30\u001b[0m\u001b]8;;\u001b\\\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cpre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"\u003e\u003cspan style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\"\u003e                    \u003c/span\u003e\u003cspan style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"\u003eINFO    \u003c/span\u003e \u003cspan style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"\u003e1523\u003c/span\u003e items loaded                                                                                   \u003ca href=\"file://\u003cipython-input-34-a1e57a40ea70\u003e:30\" target=\"_blank\"\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e\u0026lt;ipython-input-34-a1e57a40ea70\u0026gt;\u003c/span\u003e\u003c/a\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e:\u003c/span\u003e\u003ca href=\"file://\u003cipython-input-34-a1e57a40ea70\u003e:30#__init__:30\" target=\"_blank\"\u003e\u003cspan style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"\u003e__init__:30\u003c/span\u003e\u003c/a\u003e\n","\u003c/pre\u003e\n"],"text/plain":["\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[2;33mINFO    \u001b[0m \u001b[1;36m1523\u001b[0m items loaded                                                                                   \u001b]8;id=132317;file://\u003cipython-input-34-a1e57a40ea70\u003e:30\u001b\\\u001b[2m\u003cipython-input-34-a1e57a40ea70\u003e\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=257335;file://\u003cipython-input-34-a1e57a40ea70\u003e:30#__init__:30\u001b\\\u001b[2m__init__:30\u001b[0m\u001b]8;;\u001b\\\n"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader\n","\n","\n","trn_ids, val_ids = train_test_split(df.ImageID.unique(), test_size=0.1, random_state=99)\n","trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n","len(trn_df), len(val_df)\n","\n","train_ds = OpenDataset(trn_df)\n","test_ds = OpenDataset(val_df)\n","\n","train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n","test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"rOJNxBdUKiIO"},"source":["##5.Define functions to train on a batch of data and calculate the accuracy and loss values on the validation data"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":406,"status":"ok","timestamp":1729852670951,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"M7AvzkZLKc9D"},"outputs":[],"source":["def train_batch(inputs, model, criterion, optimizer):\n","  model.train()\n","  N = len(train_loader)\n","  images, boxes, labels = inputs\n","  _regr, _clss = model(images)\n","  loss = criterion(_regr, _clss, boxes, labels)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","  return loss\n","\n","\n","@torch.no_grad()\n","def validate_batch(inputs, model, criterion):\n","  model.eval()\n","  images, boxes, labels = inputs\n","  _regr, _clss = model(images)\n","  loss = criterion(_regr, _clss, boxes, labels)\n","  return loss"]},{"cell_type":"markdown","metadata":{"id":"1Okw1QNGLx_K"},"source":["##6.Import the model"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1729852485643,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"MLswKHqZLt6Q"},"outputs":[],"source":["from model import SSD300, MultiBoxLoss\n","from detect import *"]},{"cell_type":"markdown","metadata":{"id":"KyoeKZN8MDMs"},"source":["##7.Initialize the model, optimizer, and loss function."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4671,"status":"ok","timestamp":1729852677506,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"4XZ-fr3cL-Tj","outputId":"c8a3264e-6f99-4635-c647-c27a98e86b98"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Loaded base model.\n","\n"]}],"source":["from torch_snippets.torch_loader import Report\n","\n","n_epochs = 3\n","\n","model = SSD300(num_classes, device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy, device=device)\n","\n","log = Report(n_epochs=n_epochs)\n","logs_to_print = 5"]},{"cell_type":"markdown","metadata":{"id":"5oV806nUNxT6"},"source":["##8.Train the model over increasing epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"KL0Qia9iMzR8"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0.107  trn_loss: 3.401  (5388.98s - 145487.71s remaining)"]}],"source":["for epoch in range(n_epochs):\n","  _n = len(train_loader)\n","  for ix, inputs in enumerate(train_loader):\n","    loss = train_batch(inputs, model, criterion, optimizer)\n","    pos = (epoch + (ix+1)/_n)\n","    log.record(pos, trn_loss=loss.item(), end='\\r')\n","\n","  _n = len(test_loader)\n","  for ix, inputs in enumerate(test_loader):\n","    loss = validate_batch(inputs, model, criterion)\n","    pos = (epoch + (ix+1)/_n)\n","    log.record(pos, val_loss=loss.item(), end='\\r')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1729852490317,"user":{"displayName":"EBIL PAUL","userId":"00176367685673316210"},"user_tz":-180},"id":"KTzNn0e5OvaT"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMVp2N2IBzXVWGvx/7jpzM6","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}