{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNf7Uj4GTdjewu6EaS3+7AX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Face Recognition Model Training with Triplet Loss\n","\n","In this notebook, we will go through the steps to create a face recognition model using triplet loss. We will cover data preparation, model definition, and the training process.\n","\n","## 1. Setup and Imports\n","\n","First, let's import the necessary libraries and set up the environment.\n"],"metadata":{"id":"sVL_0aSGSmmr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsWgXAZKSkKc"},"outputs":[],"source":["# Import necessary libraries\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"markdown","source":["## 2. Data Preparation\n","\n","We will prepare the data by organizing it into triplets (anchor, positive, negative).\n","\n","### 2.1 Organizing the Data\n","\n","Ensure your data is organized in the following structure:\n","\n","/dataset/\n","\n","    /dataset/  \n","      /person1/\n","          img1.jpg\n","          img2.jpg\n","          ...\n","      /person2/\n","          img1.jpg\n","          img2.jpg\n","          ...\n"],"metadata":{"id":"BIPh8iL7SxES"}},{"cell_type":"code","source":["\n","def create_triplets(dataset_path, num_triplets=1000):\n","  \"\"\"\n","  Create triplets for training from the dataset.\n","\n","  Parameters:\n","  - dataset_path (str): Path to the dataset directory.\n","  - num_triplets (int): Number of triplets to generate.\n","\n","  Returns:\n","  - List of triplets (anchor, positive, negative) paths.\n","  \"\"\"\n","  triplets = []\n","  person_folders = [f.path for f in os.scandir(dataset_path) if f.is_dir()]\n","\n","  for _ in range(num_triplets):\n","    anchor_person = random.choice(person_folders)\n","    anchor_images = os.listdir(anchor_person)\n","\n","    if len(anchor_images) < 2:\n","      continue  # Skip if there are not enough images\n","\n","    anchor = random.choice(anchor_images)\n","    positive = random.choice([img for img in anchor_images if img != anchor])\n","\n","    negative_person = random.choice([p for p in person_folders if p != anchor_person])\n","    negative = random.choice(os.listdir(negative_person))\n","\n","    triplets.append((os.path.join(anchor_person, anchor),\n","                      os.path.join(anchor_person, positive),\n","                      os.path.join(negative_person, negative)))\n","\n","  return triplets\n"],"metadata":{"id":"ddWTvUC1TMIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Dataset Class\n","\n","Next, we'll create a custom dataset class to load the triplet images and apply necessary transformations.\n"],"metadata":{"id":"wI-viQ0HTXoe"}},{"cell_type":"code","source":["class TripletDataset(Dataset):\n","  \"\"\"\n","  Custom Dataset class for loading triplet images.\n","  \"\"\"\n","\n","  def __init__(self, triplet_list):\n","    self.triplet_list = triplet_list\n","    self.transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Resize images to 224x224\n","        transforms.ToTensor(),           # Convert images to tensor\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n","    ])\n","\n","  def __len__(self):\n","    return len(self.triplet_list)\n","\n","  def __getitem__(self, idx):\n","    anchor_path, positive_path, negative_path = self.triplet_list[idx]\n","    anchor_image = self.load_image(anchor_path)\n","    positive_image = self.load_image(positive_path)\n","    negative_image = self.load_image(negative_path)\n","    return anchor_image, positive_image, negative_image\n","\n","  def load_image(self, image_path):\n","    \"\"\"\n","    Load and transform the image from the specified path.\n","    \"\"\"\n","    image = Image.open(image_path).convert('RGB')  # Load image and convert to RGB\n","    return self.transform(image)  # Apply transformations\n"],"metadata":{"id":"o8_zgtrATY7t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.3 Creating DataLoader\n","\n","Now, let's create the triplet list and the DataLoader for training.\n"],"metadata":{"id":"QV8dhSQnTdnr"}},{"cell_type":"code","source":["# Create triplet data\n","triplet_list = create_triplets('path_to_your_dataset', num_triplets=1000)\n","\n","# Create dataset and DataLoader with shuffling\n","train_dataset = TripletDataset(triplet_list)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Shuffle data\n"],"metadata":{"id":"P3enkQ7wTgRW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Model Definition\n","\n","Now, we'll define our face recognition model. We'll use a simple Convolutional Neural Network (CNN) that outputs embeddings for the images.\n"],"metadata":{"id":"cMR57DA9TkU5"}},{"cell_type":"code","source":["class FaceRecognitionModel(nn.Module):\n","  \"\"\"\n","  A simple CNN model for face recognition.\n","  \"\"\"\n","\n","  def __init__(self):\n","    super(FaceRecognitionModel, self).__init__()\n","    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","    self.fc1 = nn.Linear(64 * 112 * 112, 128)  # Adjust based on input size\n","    self.fc2 = nn.Linear(128, 128)  # Output embedding size\n","\n","  def forward(self, x):\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = x.view(x.size(0), -1)  # Flatten the tensor\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)  # Embedding output\n","    return x\n"],"metadata":{"id":"Agv35zQMTnab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Triplet Loss Function\n","\n","We will implement the triplet loss function that encourages the model to minimize the distance between the anchor and positive embeddings while maximizing the distance between the anchor and negative embeddings.\n"],"metadata":{"id":"LDUaqI5LTrep"}},{"cell_type":"code","source":["def triplet_loss(anchor, positive, negative, margin=1.0):\n","  \"\"\"\n","  Compute the triplet loss.\n","\n","  Parameters:\n","  - anchor: Embedding for anchor image.\n","  - positive: Embedding for positive image.\n","  - negative: Embedding for negative image.\n","  - margin: Margin for triplet loss.\n","\n","  Returns:\n","  - Computed loss.\n","  \"\"\"\n","  pos_distance = torch.nn.functional.pairwise_distance(anchor, positive)\n","  neg_distance = torch.nn.functional.pairwise_distance(anchor, negative)\n","  loss = torch.mean(torch.clamp(pos_distance - neg_distance + margin, min=0.0))\n","  return loss\n"],"metadata":{"id":"LaYIHfeuTtjs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Training the Model\n","\n","We will now implement the training function, which will iterate over the dataset and optimize the model using the triplet loss.\n"],"metadata":{"id":"50Wbx3I4Txsh"}},{"cell_type":"code","source":["def train_model(model, train_loader, optimizer, num_epochs, device):\n","  \"\"\"\n","  Train the face recognition model.\n","\n","  Parameters:\n","  - model: The face recognition model.\n","  - train_loader: DataLoader for training data.\n","  - optimizer: Optimizer for training.\n","  - num_epochs: Number of epochs to train.\n","  - device: Device to run the model on (CPU or GPU).\n","  \"\"\"\n","  model.train()  # Set the model to training mode\n","  for epoch in range(num_epochs):\n","    total_loss = 0.0\n","    for anchor, positive, negative in train_loader:\n","      anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n","\n","      optimizer.zero_grad()\n","      anchor_embedding = model(anchor)\n","      positive_embedding = model(positive)\n","      negative_embedding = model(negative)\n","\n","      loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n","      loss.backward()\n","      optimizer.step()\n","\n","      total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"],"metadata":{"id":"vO9r5UkoTz88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Putting It All Together\n","\n","Now, let's run everything together: we will initialize the model, define the optimizer, and start training.\n"],"metadata":{"id":"x4H4N1IST3WN"}},{"cell_type":"code","source":["# Initialize the model, optimizer\n","model = FaceRecognitionModel().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n","\n","# Set the number of epochs\n","num_epochs = 20  # Adjust as necessary\n","\n","# Start training\n","train_model(model, train_loader, optimizer, num_epochs, device)\n"],"metadata":{"id":"7iovLFVJT5j1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion\n","\n","In this notebook, we created a face recognition model using triplet loss. We covered data preparation, model definition, loss calculation, and training. This basic framework can be expanded upon with more complex models, data augmentation, and optimization strategies for improved performance.\n","\n","Summary\n","* Data Preparation: Organized images into triplet format.\n","* Model Definition: Implemented a CNN for face embeddings.\n","* Loss Function: Used triplet loss for training.\n","* Training: Monitored the model's performance during training."],"metadata":{"id":"HzeqsiaVT-ve"}},{"cell_type":"code","source":[],"metadata":{"id":"AdA-NWA8UEv2"},"execution_count":null,"outputs":[]}]}